{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOypXKoLIaJcYmx+jyV3Mx3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Neural Machine Translation **(NMT)** is an approach to Machine Translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modelling entire sentences in a single integrated model."],"metadata":{"id":"UfOrdYJgrImY"}},{"cell_type":"markdown","source":["SO the Sequence (seq2seq) model in this post uses an encoder decoder architecture, which uses a type of RNN called LSTM (Long short Term Memory), where the encoder neural network encodes the input language ssquence into a single vector called a ***Context Vector***.\n","\n","This *Context Vector* is said to contain the abstract representation of the input language sequence.\n","\n","This vector is then passed into the decoder neural network, which is used to output the corresponding output language translation sentence, one word at a time."],"metadata":{"id":"uA1mzX4m0-86"}},{"cell_type":"markdown","source":["***Torch text*** is a powerful library for making the text data ready for a variety of NLP tasks. It has all the tools to perform preprocessing on the textual data.\n","\n","1. Fields :\n","This is a class under the torch text, where we specify how the preprocessing should be done on our data corpus.\n","\n","2. TabularDataset :\n","Using this class, we can actually define the Dataset of columns stored in CSV, TSV, or JSON format and also map them into integers.\n","\n","3. BucketIterator :\n","Using this class, we can perform padding our data for approximation and make batches with our data for model training."],"metadata":{"id":"p2bin69L3OpC"}},{"cell_type":"code","source":["!pip install -U torchtext==0.6.0\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","import numpy as np\n","import pandas as pd\n","import spacy, random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43SzVUKW-ku2","executionInfo":{"status":"ok","timestamp":1673946963996,"user_tz":-330,"elapsed":15118,"user":{"displayName":"Akash Kumar Singh","userId":"15410083847232088767"}},"outputId":"05c3dd3e-c33c-4c3a-8b0b-7008c86a8af8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.6.0) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.6.0) (2.25.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.6.0) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchtext==0.6.0) (1.13.0+cu116)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.6.0) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.6.0) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchtext==0.6.0) (4.4.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.14.0\n","    Uninstalling torchtext-0.14.0:\n","      Successfully uninstalled torchtext-0.14.0\n","Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}]},{"cell_type":"code","source":["## Loading the SpaCy's vocabulary for our desired languages. \n","!python -m spacy download en_core_web_sm --quiet\n","!python -m spacy download de_core_news_sm --quiet\n","spacy_german = spacy.load(\"de_core_news_sm\")\n","spacy_english = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"SKIwRbpJ-m8z","executionInfo":{"status":"ok","timestamp":1673946997486,"user_tz":-330,"elapsed":29610,"user":{"displayName":"Akash Kumar Singh","userId":"15410083847232088767"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60a9a044-4dcf-4115-ca13-3922f49a965e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","2023-01-17 09:16:12.568043: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","2023-01-17 09:16:28.516773: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"markdown","source":["***Data PreProcessing***"],"metadata":{"id":"H4G5eBlZIm4Z"}},{"cell_type":"code","source":["def tokenize_german(text):\n","  return [token.text for token in spacy_german.tokenizer(text)]\n","\n","def tokenize_english(text):\n","  return [token.text for token in spacy_english.tokenizer(text)]\n","\n","german = Field(tokenize=tokenize_german, lower=True, init_token=\"<sos>\",\n","               eos_token=\"<eos>\")\n","\n","english = Field(tokenize=tokenize_english, lower=True, init_token=\"<sos>\",\n","               eos_token=\"<eos>\")\n","\n","train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),\n","                                                    fields = (german, english))\n","\n","german.build_vocab(train_data, max_size=10000, min_freq=3)\n","english.build_vocab(train_data, max_size=10000, min_freq=3)\n","\n","print(f\"Unique tokens in source (de) vocabulary: {len(german.vocab)}\")\n","print(f\"Unique tokens in source (en) vocabulary: {len(english.vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIYeWrmOALoT","executionInfo":{"status":"ok","timestamp":1673948931380,"user_tz":-330,"elapsed":3897,"user":{"displayName":"Akash Kumar Singh","userId":"15410083847232088767"}},"outputId":"b796d31b-dce4-4506-ab43-039072f30e3e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in source (de) vocabulary: 5374\n","Unique tokens in source (en) vocabulary: 4556\n"]}]},{"cell_type":"markdown","source":["***Data Processing***"],"metadata":{"id":"FTTso_veIve7"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","BATCH_SIZE = 32\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n","                                                                      batch_size=BATCH_SIZE,\n","                                                                      sort_within_batch=True,\n","                                                                      sort_key=lambda x: len(x.src),\n","                                                                      device = device)\n"],"metadata":{"id":"6u8a19BAIZAl","executionInfo":{"status":"ok","timestamp":1673950151059,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akash Kumar Singh","userId":"15410083847232088767"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_iterator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wy3uqczZND4W","executionInfo":{"status":"ok","timestamp":1673950242512,"user_tz":-330,"elapsed":12,"user":{"displayName":"Akash Kumar Singh","userId":"15410083847232088767"}},"outputId":"b535c633-7eee-4e0e-9f61-87f4e00ac0a5"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torchtext.data.iterator.BucketIterator at 0x7efd09788e50>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"s_--wCa8NaKK"},"execution_count":null,"outputs":[]}]}